# main.py
import argparse
import os
import sys
import random
from pathlib import Path
from typing import List

import numpy as np
import pandas as pd
from openai import OpenAI

from app.retriever import SimpleIndex
from app.chatbot import answer_naive, answer_multiquery, answer_hyde
from app.config import load_env, assert_env  # assert_envê°€ ì—†ë‹¤ë©´ load_envë§Œ ì‚¬ìš©í•˜ì„¸ìš”
from app.question_manager import QuestionManager
from app.ranker import EmotionKeywordAnalyzer
from app.report_generator import generate_final_report

# -------------------- prompting ëª¨ë“ˆ ì„í¬íŠ¸ (VALIDATE_SYSTEM ë¯¸ì •ì˜ ëŒ€ë¹„) --------------------
# NEXT_QUESTION_SYSTEM, REFINE_QUESTION_SYSTEM ë“±ì€ ê¸°ì¡´ëŒ€ë¡œ ì‚¬ìš©í•˜ë˜,
# VALIDATE_SYSTEMì´ ì•„ì§ ì—†ë‹¤ë©´ ì•„ë˜ì—ì„œ ì•ˆì „í•œ ê¸°ë³¸ê°’ì„ ì±„ì›Œë„£ìŠµë‹ˆë‹¤.
try:
    from app.prompting import (
        NEXT_QUESTION_SYSTEM,
        build_next_question_user_prompt,
        REFINE_QUESTION_SYSTEM,
        build_refine_question_user_prompt,
    )
    try:
        from app.prompting import VALIDATE_SYSTEM  # ì—†ì„ ìˆ˜ ìˆìŒ
    except Exception:
        VALIDATE_SYSTEM = None
except Exception:
    # prompting ëª¨ë“ˆ ì „ì²´ê°€ ì—†ì„ ê°€ëŠ¥ì„±ì€ ë‚®ì§€ë§Œ, ë°©ì–´ì ìœ¼ë¡œ ìµœì†Œ í”„ë¡¬í”„íŠ¸ë¥¼ ë‘¡ë‹ˆë‹¤.
    NEXT_QUESTION_SYSTEM = "ë„ˆëŠ” ê³µê°í˜• ìƒë‹´ê°€ë‹¤. ì‚¬ìš©ìì˜ ì§ì „ ë‹µë³€ì„ ë°›ì•„ ë‹¤ìŒ í•œ ë¬¸ì¥ ì§ˆë¬¸ë§Œ ì¶œë ¥í•œë‹¤."
    REFINE_QUESTION_SYSTEM = "ë¬¸ì¥ì„ ë¶€ë“œëŸ½ê³  ê³µê° ìˆê²Œ ë‹¤ë“¬ë˜, í•œ ë¬¸ì¥ ì§ˆë¬¸ìœ¼ë¡œ ìœ ì§€í•œë‹¤."
    def build_next_question_user_prompt(t: str) -> str:
        return f"ë‹¤ìŒ ëŒ€í™”ë¬¸ë§¥ì„ ì½ê³  í•œ ë¬¸ì¥ ì§ˆë¬¸ë§Œ ì¶œë ¥:\n{t}"
    def build_refine_question_user_prompt(q: str) -> str:
        return f"ë‹¤ìŒ ë¬¸ì¥ì„ ë” ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸ í•œ ë¬¸ì¥ìœ¼ë¡œ ë‹¤ë“¬ì–´ì¤˜:\n{q}"
    VALIDATE_SYSTEM = None  # ì•„ë˜ ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´

#=========== ê²½ê³  ë¬¸êµ¬, ë””ë²„ê¹… ë¬¸êµ¬ ì œê±° ===========
import warnings, logging
warnings.filterwarnings("ignore", category=FutureWarning, module="torch")
try:
    from transformers.utils import logging as hf_logging
    hf_logging.set_verbosity_error()
except Exception:
    pass

# ============ ìƒìˆ˜/í—¬í¼ ============
PROMPT_PREFIX = "ì•„ë˜ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§€ëŠ” ë‹¤ìŒ ì§ˆë¬¸ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.\n"

WARNING_MESSAGE = "ë¹„ì •ìƒì ì¸ ë‹¨ì–´ê°€ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤.ì±„íŒ…ì„ ë‹¤ì‹œ ì‹œì‘í•´ì£¼ì„¸ìš”."

CRISIS_MSG = (
    "ì§€ê¸ˆ ë§ì´ í˜ë“œì‹  ê²ƒ ê°™ì•„ìš”. ì €ëŠ” ì‘ê¸‰ ìƒí™©ì„ ì§ì ‘ ì²˜ë¦¬í•  ìˆ˜ëŠ” ì—†ì§€ë§Œ, "
    "ì•ˆì „ì´ ê°€ì¥ ì¤‘ìš”í•´ìš”. ëŒ€í•œë¯¼êµ­ì— ê³„ì‹œë‹¤ë©´ 112(ê¸´ê¸‰) ë˜ëŠ” êµ­ë²ˆ ì—†ì´ 1393(ìì‚´ì˜ˆë°© ìƒë‹´, 24ì‹œê°„)ìœ¼ë¡œ "
    "ì§€ê¸ˆ ë°”ë¡œ ì—°ë½í•´ ì£¼ì„¸ìš”. ê°€ê¹Œìš´ ë¶„ì´ë‚˜ ì „ë¬¸ê¸°ê´€ì— ì¦‰ì‹œ ë„ì›€ì„ ìš”ì²­í•˜ëŠ” ê²ƒë„ í° ë„ì›€ì´ ë©ë‹ˆë‹¤. "
    "í•„ìš”í•œ ì§€ì›ì„ ë°›ìœ¼ì‹¤ ìˆ˜ ìˆë„ë¡ ëŒ€í™”ë¥¼ ì ì‹œ ì¤‘ë‹¨í• ê²Œìš”."
)

DISTRESS_MSG = (
    "í˜ë“  ë§ˆìŒì„ ë‚˜ëˆ  ì£¼ì…”ì„œ ê³ ë§ˆì›Œìš”. ì›í•˜ì‹œë©´ ì „ë¬¸ ìƒë‹´ê³¼ ì—°ê²°ì„ ë„ì™€ë“œë¦´ê²Œìš”. "
    "ê´œì°®ë‹¤ë©´ ì§€ê¸ˆ ëŠë¼ëŠ” ê°ì •ì„ ì¡°ê¸ˆ ë” ì´ì•¼ê¸°í•´ë„ ì¢‹ì•„ìš”."
    "ìƒë‹´ì‚¬ ì—°ê²°ì„ ì›í•˜ì‹œë©´ 'ìƒë‹´ì‚¬ ì—°ê²°'ì´ë¼ê³  ì…ë ¥í•´ ì£¼ì„¸ìš”."
)

SUPPORT_MSG = (
    "ì „ë¬¸ ìƒë‹´ì‚¬ì™€ ì§ì ‘ ì—°ê²°í•´ë“œë¦´ ìˆ˜ëŠ” ì—†ì§€ë§Œ, ë„ì›€ì„ ë°›ì„ ìˆ˜ ìˆëŠ” ê³µì‹ ì±„ë„ì´ ìˆìŠµë‹ˆë‹¤:\n"
    "- ë³´ê±´ë³µì§€ë¶€ ìì‚´ì˜ˆë°©ìƒë‹´ì „í™”: 1393 (24ì‹œê°„, ë¬´ë£Œ)\n"
    "- ì •ì‹ ê±´ê°• ìƒë‹´ ì „í™”: 1577-0199\n"
    "- ë³´ê±´ë³µì§€ìƒë‹´ì„¼í„°: 129\n\n"
    "ê°€ê¹Œìš´ ì •ì‹ ê±´ê°•ë³µì§€ì„¼í„°ë‚˜ ì •ì‹ ê±´ê°•ì˜í•™ê³¼ì—ì„œ ì „ë¬¸ì ì¸ ë„ì›€ì„ ë°›ì•„ë³´ì‹œëŠ” ê²ƒë„ í° í˜ì´ ë  ìˆ˜ ìˆì–´ìš”."
)

# -------------------- VALIDATE_SYSTEM ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ (ì—†ì„ ë•Œë§Œ ì£¼ì…) --------------------
if not VALIDATE_SYSTEM:
    VALIDATE_SYSTEM = """
ë„ˆëŠ” í•œêµ­ì–´ ëŒ€í™”ì˜ ì•ˆì „ì„± ê²€ì¦ê¸°ë‹¤. ì…ë ¥ëœ 'ì‚¬ìš©ì' ë¬¸ì¥ì„ ë‹¤ìŒ 4ê°€ì§€ ì¤‘ í•˜ë‚˜ë¡œ **ì •í™•íˆ í•œ ì¤„**ë¡œë§Œ ë¶„ë¥˜í•´ ì¶œë ¥í•œë‹¤.

[ì¶œë ¥ê°’(ì •í™•íˆ ì•„ë˜ ì¤‘ í•˜ë‚˜ë§Œ)]
OK
ESCALATE:DISTRESS
ESCALATE:CRISIS
BLOCK

[ë¶„ë¥˜ ê¸°ì¤€]
1) ESCALATE:CRISIS  â† ì¦‰ì‹œ ìœ„í—˜ ì‹ í˜¸(ì/íƒ€í•´ ì˜ë„Â·ê³„íšÂ·êµ¬ì²´ì  í‘œí˜„, ê°•ë ¥ ìœ„í˜‘)
   ì˜ˆ: "ì£½ê³  ì‹¶ì–´ìš”", "ìì‚´í•  ê±°ì˜ˆìš”", "ì§€ê¸ˆ ë² ê³  ì‹¶ë‹¤", "ë„ˆ ì£½ì¼ ê±°ì•¼"
2) ESCALATE:DISTRESS â† ê³ í†µ/ìš°ìš¸/ë¶ˆì•ˆ ë“± ì •ì„œì  ìœ„í—˜ ì‹ í˜¸(ê³„íšÂ·ì˜ë„Â·ìœ„í˜‘ì€ ì—†ìŒ)
   ì˜ˆ: "ìš°ìš¸í•´ìš”", "ì‚¬ëŠ” ê²Œ í˜ë“¤ë‹¤", "ë¶ˆì•ˆí•´ì„œ ì ì´ ì•ˆ ì™€ìš”"
3) BLOCK â† ì•„ë˜ ê¸ˆì§€ í•­ëª©ì— í•´ë‹¹
   - ë¯¸ì„±ë…„ì ê´€ë ¨ ì„±ì  ë‚´ìš©/ì°©ì·¨
   - ë¶ˆë²•Â·ìœ„í—˜ í–‰ìœ„ ë°©ë²•ì˜ êµ¬ì²´ ì§€ì‹œ/ìš”ì²­(í­íƒ„Â·ë§ˆì•½ ì œì¡°ë²• ë“±)
   - ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸/ê³„ì¢Œ/ì¹´ë“œë²ˆí˜¸/ì •í™•í•œ ì§‘ì£¼ì†Œ/ë¡œê·¸ì¸ ë¹„ë²ˆ ë“± ë¯¼ê°ì‹ë³„ì •ë³´
   - ì¸ì¢…/ì„±ë³„/ì„±ì ì§€í–¥/ì¥ì•  ë“±ì— ëŒ€í•œ í˜ì˜¤ ì„ ë™, ë…¸ê³¨ì  ëª¨ìš•Â·í˜‘ë°•ì˜ ë°˜ë³µ
4) OK â† ìœ„ ì¡°ê±´ë“¤(1~3)ì— í•´ë‹¹í•˜ì§€ ì•Šìœ¼ë©´ ëª¨ë‘ OK. ì• ë§¤í•˜ë©´ OK.

[ì¤‘ìš”]
- ë°˜ë“œì‹œ ìœ„ 4ê°œ ì¤‘ í•˜ë‚˜ë§Œ ê·¸ëŒ€ë¡œ ì¶œë ¥í•œë‹¤. ì„¤ëª…/ë§ˆí¬ë‹¤ìš´/ë”°ì˜´í‘œ/ì´ëª¨ì§€ ê¸ˆì§€.
""".strip()


def resolve_sct_path(user_path: str, root: Path) -> Path:
    """SCT ì§ˆë¬¸ í…œí”Œë¦¿ ê²½ë¡œ ì•ˆì „í™”"""
    p = Path(user_path)
    cands = [p if p.is_absolute() else (root / p)]
    # í”í•œ ìœ„ì¹˜ í›„ë³´
    cands += [
        root / "sct_questions.jsonl",
        root.parent / "sct_questions.jsonl",
    ]
    for c in cands:
        if c.exists():
            return c
    raise FileNotFoundError(
        "SCT ì§ˆë¬¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œë„í•œ ê²½ë¡œ:\n  " + "\n  ".join(str(x) for x in cands)
    )


def resolve_analysis_csv(user_path: str, root: Path) -> Path:
    """ë¦¬í¬íŠ¸ìš© ë¶„ì„ CSV ê²½ë¡œ ìë™ íƒìƒ‰"""
    p = Path(user_path)
    cands = [p if p.is_absolute() else (root / p)]
    cands += [
        root / "Data" / "train.csv",
        root / "Data" / "train_data.csv",
        root.parent / "Data" / "train.csv",
        root.parent / "Data" / "train_data.csv",
    ]
    for c in cands:
        if c.exists():
            return c
    raise FileNotFoundError(
        "ë¶„ì„ìš© CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œë„í•œ ê²½ë¡œ:\n  " + "\n  ".join(str(x) for x in cands)
    )

def build_next_question(client: OpenAI, gen_model: str, history: List[dict], fallback_pool: List[str]) -> str:
    """ì§ì „ 2í„´ ë¬¸ë§¥ì„ ì‚¬ìš©í•´ì„œ ë‹¤ìŒ ì§ˆë¬¸ 1ë¬¸ì¥ ìƒì„±"""
    ctx = "".join(
        f"ìƒë‹´ê°€: {conv['question']}\nì‚¬ìš©ì: {conv['answer']}\n"
        for conv in history[-2:]
    )
    prompt = f"{PROMPT_PREFIX}{ctx}\nìƒë‹´ê°€:"
    resp = client.chat.completions.create(
        model=gen_model,
        messages=[
            {"role": "system", "content": "í•œêµ­ì–´ ì‹¬ë¦¬ ìƒë‹´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt},
        ],
        temperature=0.7,
        max_tokens=100,
    )
    q = (resp.choices[0].message.content or "").strip()
    return q if q else random.choice(fallback_pool)


# ============ ë¦¬í¬íŠ¸ìš© ê²½ëŸ‰ RAG (íŒŒì¼ ë‚´ í¬í•¨) ============
class _MiniDoc:
    def __init__(self, text: str):
        self.page_content = text

class MiniRAG:
    """Response í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë§Œìœ¼ë¡œ ê°„ë‹¨ ìœ ì‚¬ë„ ê²€ìƒ‰í•˜ëŠ” ê²½ëŸ‰ RAG"""
    def __init__(self, texts: List[str], client: OpenAI, embed_model: str):
        self.texts = texts
        self.client = client
        self.embed_model = embed_model
        self.vecs = self._embed(texts)  # (N, D) L2 ì •ê·œí™”

    def _embed(self, texts: List[str]) -> np.ndarray:
        BATCH = 100  # í•œë²ˆì— 100ê°œì”©
        arrs = []
        for i in range(0, len(texts), BATCH):
            chunk = texts[i:i+BATCH]
            embs = self.client.embeddings.create(model=self.embed_model, input=chunk).data
            arrs.append(np.array([e.embedding for e in embs], dtype="float32"))
        arr = np.vstack(arrs)
        arr /= (np.linalg.norm(arr, axis=1, keepdims=True) + 1e-8)
        return arr

    def similarity_search(self, query: str, k: int = 1) -> List[_MiniDoc]:
        q = self._embed([query])[0]
        sims = self.vecs @ q
        top = sims.argsort()[-k:][::-1]
        return [_MiniDoc(self.texts[i]) for i in top]


# -------------------- ì•ˆì „ì„± ê²€ì¦ í—¬í¼ (LLM ì¶œë ¥ ê³ ì •) --------------------
def _normalize_verdict(text: str) -> str:
    t = (text or "").strip().upper()
    return t

def validate_message(client: OpenAI, gen_model: str, current_question: str, user_response: str) -> str:
    """
    ë°˜í™˜: 'OK' | 'ESCALATE:DISTRESS' | 'ESCALATE:CRISIS' | 'BLOCK' | 'UNKNOWN' | 'ERROR'
    """
    val_query = f"í˜„ì¬ ì§ˆë¬¸: {current_question}\nì‚¬ìš©ì: {user_response}\nì´ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ìœ„ ì§€ì¹¨ìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”."
    try:
        res = client.chat.completions.create(
            model=gen_model,
            messages=[
                {"role": "system", "content": VALIDATE_SYSTEM},
                {"role": "user", "content": val_query},
            ],
            temperature=0.0,
            max_tokens=8,
        )
        raw = (res.choices[0].message.content or "")
        verdict = _normalize_verdict(raw)
        if verdict in {"OK", "ESCALATE:DISTRESS", "ESCALATE:CRISIS", "BLOCK"}:
            return verdict
        return "UNKNOWN"
    except Exception as e:
        print(f"âš ï¸ ê²€ì¦ í˜¸ì¶œ ì˜¤ë¥˜: {e}")
        return "ERROR"


# ============ ë©”ì¸ ============
def main():
    # ---- ì¸ì
    ap = argparse.ArgumentParser()
    ap.add_argument("--mode", choices=["naive", "multiquery", "hyde", "sct"], default="sct")
    ap.add_argument("--mq", type=int, default=4, help="multiquery ê°œìˆ˜")
    ap.add_argument("--sct_questions", type=str, default="../sct_questions.jsonl", help="SCT ì§ˆë¬¸ í…œí”Œë¦¿ ê²½ë¡œ")
    ap.add_argument("--analysis_csv", type=str, default="../Data/train.csv", help="ë¶„ì„ìš© RAG csv ê²½ë¡œ")
    args = ap.parse_args()

    # ---- ë£¨íŠ¸/ê²½ë¡œ
    ROOT = Path(__file__).resolve().parent
    sct_path = resolve_sct_path(args.sct_questions, ROOT)
    analysis_csv_path = resolve_analysis_csv(args.analysis_csv, ROOT)

    # ---- í™˜ê²½ & í´ë¼ì´ì–¸íŠ¸
    env = load_env()
    try:
        assert_env()  # ì—†ìœ¼ë©´ exceptë¡œ ë¬´ì‹œ
    except Exception:
        pass

    openai_api_key = env["OPENAI_API_KEY"]
    embed_model = env["EMBED_MODEL"]
    gen_model = env["GEN_MODEL"]
    client = OpenAI(api_key=openai_api_key)

    # ---- ê¸°ë³¸ ì¸ë±ìŠ¤ (ëŒ€í™” RAG)
    index = SimpleIndex("index")  # index/vectors.npz, meta.json í•„ìš”

    # ===================== SCT ëª¨ë“œ =====================
    if args.mode == "sct":
        print("ğŸ§  SCT 5í„´ ëŒ€í™” ë° ë¦¬í¬íŠ¸ ëª¨ë“œ")

        # 1) ì§ˆë¬¸ í…œí”Œë¦¿ ë¡œë“œ
        qm = QuestionManager(str(sct_path))

        # 2) ì‚¬ìš©ì ì •ë³´
        def info(i):
            if i == 1:
                print("ì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”")
            elif i == 2:
                print("ë‚˜ì´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”")
            elif i == 3:
                print("ì„±ë³„ì„ ì…ë ¥í•´ì£¼ì„¸ìš” (ë‚¨/ì—¬/ê¸°íƒ€)")
            return i+1
        # 2) ì‚¬ìš©ì ì •ë³´
        print("ìƒë‹´ì„ ì‹œì‘í•˜ê¸° ì „ì— ëª‡ ê°€ì§€ ì •ë³´ë¥¼ ì—¬ì­¤ë³¼ê²Œìš”. ì´ë¦„, ë‚˜ì´, ì„±ë³„ì„ ìˆœì°¨ì ìœ¼ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        # ì‚¬ìš©ì ì •ë³´ ì…ë ¥ ë£¨í”„
        # (ì´ë¦„, ë‚˜ì´, ì„±ë³„)
        # - ì´ë¦„: 1~20ì, í•œê¸€/ì˜ë¬¸/ìˆ«ìë§Œ í—ˆìš©
        # - ë‚˜ì´: 0~120, ìˆ«ìë§Œ í—ˆìš©
        # - ì„±ë³„: ë‚¨/ì—¬/ê¸°íƒ€ ì¤‘ í•˜ë‚˜
        while True:
            try:
                print("ì´ë¦„ì€ 1~20ì ì‚¬ì´ì˜ í•œê¸€/ì˜ë¬¸/ìˆ«ìë§Œ í—ˆìš©ë©ë‹ˆë‹¤.")
                name = input().strip()
                if not (1 <= len(name) <= 20) or not all(c.isalnum() or c in "ê°€-í£" for c in name):
                    raise ValueError("ì´ë¦„ì€ 1~20ì ì‚¬ì´ì˜ í•œê¸€/ì˜ë¬¸/ìˆ«ìë§Œ í—ˆìš©ë©ë‹ˆë‹¤.")
                break
            except ValueError as e:
                print(f"âš ï¸ {e}. ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.")

        while True:
            try:
                print("ë‚˜ì´ëŠ” 0~120 ì‚¬ì´ì˜ ìˆ«ìì—¬ì•¼ í•©ë‹ˆë‹¤.")
                age = int(input().strip())
                if not (0 <= age <= 120):
                    raise ValueError("ë‚˜ì´ëŠ” 0~120 ì‚¬ì´ì˜ ìˆ«ìì—¬ì•¼ í•©ë‹ˆë‹¤.")
                break
            except ValueError as e:
                print(f"âš ï¸ {e}. ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.")

        while True:
            try:
                print("ì„±ë³„ì€ 'ë‚¨', 'ì—¬', 'ê¸°íƒ€' ì¤‘ í•˜ë‚˜ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                sex = input().strip()
                if not ('ë‚¨' or 'ì—¬' or 'ê¸°íƒ€'):
                    raise ValueError("ì„±ë³„ì„ ì…ë ¥í•´ì£¼ìƒˆìš” (ë‚¨/ì—¬/ê¸°íƒ€)")
                break
            except ValueError as e:
                print(f"âš ï¸ {e}. ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        user_info = {
            "name": name,
            "age": age,
            "gender": sex,
        }


        # 3) ì¹´í…Œê³ ë¦¬ ì„ íƒ
        cats = qm.get_categories()
        print("\n")
        print("ì–´ë–¤ ì£¼ì œì— ëŒ€í•´ ì´ì•¼ê¸°í•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”? ë²ˆí˜¸ë¥¼ ì ì–´ì£¼ì„¸ìš”")
        for i, cat in enumerate(cats):
            print(f"  [{i+1}] {cat}")
        while True:
            try:
                choice = int(input())
                if 1 <= choice <= len(cats):
                    selected_category = cats[choice - 1]
                    break
                print("ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì„ íƒí•´ì£¼ì„¸ìš”.")
            except ValueError:
                print("ìˆ«ìë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        print(f"\n'{selected_category}' ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ì…¨ìŠµë‹ˆë‹¤.")

        question_pool = qm.get_questions_by_category(selected_category)
        conversation_history = []
        analyzer = EmotionKeywordAnalyzer()
        current_question = random.choice(question_pool)
        seed_q = current_question

        # ì‹œë“œ ì§ˆë¬¸ì„ í•œ ë²ˆ ë‹¤ë“¬ê¸°
        ref = client.chat.completions.create(
            model=gen_model,
            messages=[
                {"role": "system", "content": REFINE_QUESTION_SYSTEM},
                {"role": "user", "content": build_refine_question_user_prompt(seed_q or current_question)},
            ],
            temperature=0.2, max_tokens=60,
        )
        current_question = (ref.choices[0].message.content or current_question).strip()

        # 4) ëŒ€í™”(ìµœëŒ€ 5í„´) â€” ê°™ì€ í„´ ìœ ì§€ ì…ë ¥ ë£¨í”„ + ì•ˆì „ì„± ê²€ì¦
        MAX_TURNS = 5
        for turn in range(MAX_TURNS):
            print(f"\nAI     (ì§ˆë¬¸ {turn+1}/{MAX_TURNS}): {current_question}")

            # ----- ì…ë ¥/ê²€ì¦ ë£¨í”„: í†µê³¼í•´ì•¼ í„´ ì¢…ë£Œ -----
            while True:
                user_response = input().strip()

                
                # ì¢…ë£Œì–´ ì²˜ë¦¬(í•œê¸€/ì˜ë¬¸)
                if user_response in {"ì¢…ë£Œ", "ê·¸ë§Œ"} or user_response.lower() in {"exit", "quit"}:
                    print("ğŸ¤– ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    # í•„ìš” ì‹œ ì—¬ê¸°ì„œ ë¦¬í¬íŠ¸ ìƒì„± íŒŒíŠ¸ë¡œ ë°”ë¡œ ì´ë™í•˜ë ¤ë©´ return ëŒ€ì‹  break/flag ì‚¬ìš©
                    # ì—¬ê¸°ì„œëŠ” ì¦‰ì‹œ ë¦¬í¬íŠ¸ ìƒì„±ìœ¼ë¡œ ì´ë™í•˜ë„ë¡ break 2ì¤‘ íƒˆì¶œ ì²˜ë¦¬
                    # (ë°”ê¹¥ ë£¨í”„ íƒˆì¶œ)
                    turn = MAX_TURNS - 1  # ë°”ê¹¥ forì˜ ë§ˆì§€ë§‰ í„´ìœ¼ë¡œ ì„¤ì •
                    conversation_history.append({
                        "turn": turn + 1,
                        "question": current_question,
                        "answer": "[ì‚¬ìš©ì ì¢…ë£Œ]",
                        "emotion_label": "neutral",
                        "emotion_score": 0.0,
                        "keywords": [],
                    })
                    raise SystemExit  # ë‚´ë¶€ while íƒˆì¶œ

                if not user_response:
                    print("âš ï¸ ë‹µë³€ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    continue  # ê°™ì€ í„´ ìœ ì§€ ì¬ì…ë ¥

                # ---- ì•ˆì „ì„± ê²€ì¦ í˜¸ì¶œ ----
                verdict = validate_message(client, gen_model, current_question, user_response)
                if verdict == "OK":
                    # ê²€ì¦ í†µê³¼ â†’ ì´ë ¥ ê¸°ë¡ í›„ ì§ˆë¬¸ ìƒì„±ìœ¼ë¡œ ì§„í–‰
                    analysis = analyzer.analyze(user_response)
                    conversation_history.append({
                        "turn": turn + 1,
                        "question": current_question,
                        "answer": user_response,
                        "emotion_label": analysis["emotion_label"],
                        "emotion_score": analysis["emotion_score"],
                        "keywords": analysis["keywords"],
                    })
                    break  # ë‚´ë¶€ while íƒˆì¶œ, ë‹¤ìŒ ì§ˆë¬¸ ìƒì„±ìœ¼ë¡œ

                elif verdict == "BLOCK" or []:
                    # ì§€ì • ê²½ê³ ë¬¸ ì¶œë ¥ í›„ ê°™ì€ í„´ì—ì„œ ì¬ì…ë ¥ ìš”êµ¬
                    print(WARNING_MESSAGE)
                    continue
                
                elif verdict.startswith("ESCALATE:"):
                    # ìœ„ê¸° ìƒí™© â†’ ì¦‰ì‹œ ìƒë‹´ ì¤‘ë‹¨ ë° ì•ˆë‚´ ë©”ì‹œì§€ ì¶œë ¥
                    if verdict == "ESCALATE:CRISIS":
                        print(CRISIS_MSG)
                        raise SystemExit
                    elif verdict == "ESCALATE:DISTRESS":
                        print(DISTRESS_MSG)
                        eval = input("ìƒë‹´ì‚¬ ì—°ê²°ì„ ì›í•˜ì‹œë©´ 'ìƒë‹´ì‚¬ ì—°ê²°'ì´ë¼ê³  ì…ë ¥í•´ì£¼ì„¸ìš”: ").strip()
                        if eval == "ìƒë‹´ì‚¬ ì—°ê²°" or eval == "ìƒë‹´ì‚¬ì—°ê²°":

                            print(SUPPORT_MSG)
                            raise SystemExit

                    # ëŒ€í™” ì¢…ë£Œ í›„ ë¦¬í¬íŠ¸ ìƒì„±ìœ¼ë¡œ ì´ë™
                    turn = MAX_TURNS - 1  # ë°”ê¹¥ forì˜ ë§ˆì§€ë§‰ í„´ìœ¼ë¡œ ì„¤ì •
                    conversation_history.append({
                        "turn": turn + 1,
                        "question": current_question,
                        "answer": "[ìœ„ê¸° ìƒí™© ì¢…ë£Œ]",
                        "emotion_label": "neutral",
                        "emotion_score": 0.0,
                        "keywords": [], 
                    })
                    # ë¦¬í¬íŠ¸ ìƒì„± íŒŒíŠ¸ë¡œ ë°”ë¡œ ì´ë™
                    break  # ë‚´ë¶€ while íƒˆì¶œ

                else:  # UNKNOWN or ERROR
                    print("âš ï¸ ê²€ì¦ ê²°ê³¼ê°€ ë¶ˆëª…í™•í•©ë‹ˆë‹¤. ë‹µë³€ì„ ë‹¤ì‹œ ì‘ì„±í•´ì£¼ì„¸ìš”.")
                    continue

            # ë‚´ë¶€ whileì—ì„œ 'ì‚¬ìš©ì ì¢…ë£Œ' ì²˜ë¦¬ë¡œ ë¹ ì ¸ë‚˜ì˜¨ ê²½ìš° ë°”ê¹¥ for ë§ˆê°
            if conversation_history and conversation_history[-1]["answer"] == "[ì‚¬ìš©ì ì¢…ë£Œ]":
                break

            # â”€â”€ ë‹¤ìŒ ì§ˆë¬¸ ìƒì„± (ì§ì „ 2í„´ ë¬¸ë§¥ ì‚¬ìš©) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            recent = conversation_history[-2:] if len(conversation_history) >= 2 else conversation_history[-1:]
            turns_text = "".join(
                f"ìƒë‹´ê°€: {c['question']}\nì‚¬ìš©ì: {c['answer']}\n" for c in recent
            )

            # 1) 1ì°¨ ì§ˆë¬¸ ìƒì„±
            gen = client.chat.completions.create(
                model=gen_model,
                messages=[
                    {"role": "system", "content": NEXT_QUESTION_SYSTEM},
                    {"role": "user", "content": build_next_question_user_prompt(turns_text)},
                ],
                temperature=0.3,             # í†¤ ì•ˆì •
                max_tokens=80,
                frequency_penalty=0.2,       # ì¤‘ë³µ ì¤„ì´ê¸°
                presence_penalty=0.0,
            )
            next_q = (gen.choices[0].message.content or "").strip()

            # 2) í›„í¸ì§‘ìœ¼ë¡œ ë” ë¶€ë“œëŸ½ê²Œ êµì •
            if next_q:
                ref = client.chat.completions.create(
                    model=gen_model,
                    messages=[
                        {"role": "system", "content": REFINE_QUESTION_SYSTEM},
                        {"role": "user", "content": build_refine_question_user_prompt(next_q)},
                    ],
                    temperature=0.2,
                    max_tokens=60,
                )
                next_q = (ref.choices[0].message.content or "").strip()

            # ë¹„ìƒì‹œ fallback
            if not next_q or len(next_q) < 3 or "?" not in next_q:
                next_q = random.choice(question_pool)

            current_question = next_q
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        # 5) ë¦¬í¬íŠ¸ìš© RAG ì¤€ë¹„(MiniRAG)
        from mini_text_400 import mini_texts

        df = pd.read_csv(analysis_csv_path)
        if "Response" not in df.columns:
            raise RuntimeError(f"[ì˜¤ë¥˜] CSVì— 'Response' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {analysis_csv_path}")

        responses = df["Response"].dropna().astype(str).tolist()
        responses_small = mini_texts(responses)  # 400ì ë‹¨ìœ„ë¡œ ì••ì¶•

        analysis_rag_db = MiniRAG(responses_small, client, embed_model)

        # 6) ë¦¬í¬íŠ¸ ìƒì„±/ì¶œë ¥
        print("\nìµœì¢… ì‹¬ë¦¬ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...")
        final_report = generate_final_report(
            conversation_history=conversation_history,
            user_info=user_info,
            analysis_rag_db=analysis_rag_db,  # similarity_search(text,k) ì§€ì›
            openai_api_key=openai_api_key,
            gen_model=gen_model,
        )

        print("\n[ê¸°ë³¸ ì •ë³´]")
        print(f"  - ì´ë¦„: {user_info['name']}")
        print(f"  - ë‚˜ì´: {user_info['age']}")
        print(f"  - ì„±ë³„: {user_info['gender']}")
        print(f"  - ì„ íƒ ì¹´í…Œê³ ë¦¬: {selected_category}\n")
        print("[ì£¼ìš” í‚¤ì›Œë“œ]")
        print(f"  ì´ë²ˆ ëŒ€í™”ì—ì„œ ìì£¼ ì–¸ê¸‰í•œ í‚¤ì›Œë“œ: {', '.join(final_report['top_keywords'])}\n")
        print("[ê°ì • íŠ¸ë Œë“œ]")
        # print(f"  ê°ì • ì ìˆ˜: {final_report['emotion_trend']}")
        print("[ì‹¬ë¦¬ í•´ì„ ë° ì¡°ì–¸]")
        print(final_report["report_text"])
        return

    # ===================== ê¸°ë³¸ RAG ì±—ë´‡ =====================
    print("ğŸ’¬ OpenAI-only RAG Chatbot")
    print("íƒ€ì´í•‘ì„ ì‹œì‘í•˜ì„¸ìš”. ì¢…ë£Œ: exit")
    while True:
        q = input("\nğŸ™‹ ì§ˆë¬¸: ").strip()
        if q.lower() in {"exit", "quit"}:
            break
        if not q:
            continue
        if args.mode == "naive":
            ans = answer_naive(index, q)
        elif args.mode == "multiquery":
            ans = answer_multiquery(index, q, mq=args.mq)
        else:
            ans = answer_hyde(index, q)
        print("\n" + ans)


if __name__ == "__main__":
    main()
